diff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py
index 07824bdd686..c47f0c76659 100644
--- a/torch/_inductor/codecache.py
+++ b/torch/_inductor/codecache.py
@@ -3264,6 +3264,15 @@ class CUDACodeCache:
             lock = FileLock(os.path.join(lock_dir, key + ".lock"), timeout=LOCK_TIMEOUT)
             with lock:
                 output_path = input_path[: -len(cls._SOURCE_CODE_SUFFIX)] + dst_file_ext
+                err_path = Path(output_path + ".failed")
+                error = None
+                try:
+                    error = err_path.read_text()
+                except:
+                    pass
+                if error is not None:
+                    print("Raising error for " + key + " which is known to fail from " + str(err_path))
+                    raise exc.CUDACompileError(["unknown"], error)
                 if not os.path.exists(output_path):
                     cmd = cuda_compile_command(
                         [input_path], output_path, dst_file_ext, extra_args
@@ -3276,6 +3285,8 @@ class CUDACodeCache:
                             cmd_parts, stderr=subprocess.STDOUT, env=os.environ
                         )
                     except subprocess.CalledProcessError as error:
+                        err_path.parent.mkdir(parents=True, exist_ok=True)
+                        err_path.write_text(str(error.output)[-256:].strip())
                         raise exc.CUDACompileError(cmd_parts, error.output) from error
                     end_time = time()
                     log_duration_msg = f"CUDA Compilation took {end_time - start_time} seconds. Compile command: {cmd}"
@@ -3355,6 +3366,15 @@ class ROCmCodeCache:
             lock = FileLock(os.path.join(lock_dir, key + ".lock"), timeout=LOCK_TIMEOUT)
             with lock:
                 output_path = input_path[: -len(cls._SOURCE_CODE_SUFFIX)] + dst_file_ext
+                err_path = Path(output_path + ".failed")
+                error = None
+                try:
+                    error = err_path.read_text()
+                except:
+                    pass
+                if error is not None:
+                    print("Raising error for " + key + " which is known to fail from " + str(err_path))
+                    raise exc.CUDACompileError(["unknown"], error)
                 if not os.path.exists(output_path):
                     cmd = rocm_compile_command(
                         [input_path], output_path, dst_file_ext, extra_args
@@ -3370,6 +3390,8 @@ class ROCmCodeCache:
                         )
                         log.debug("Compilation output: %s", output)
                     except subprocess.CalledProcessError as error:
+                        err_path.parent.mkdir(parents=True, exist_ok=True)
+                        err_path.write_text(str(error.output)[-256:].strip())
                         raise exc.CUDACompileError(cmd_parts, error.output) from error
                     end_time = time()
                     log_duration_msg = f"Compilation took {end_time - start_time} seconds. Compile command: {cmd}"
diff --git a/torch/_inductor/codegen/common.py b/torch/_inductor/codegen/common.py
index 65ffc522a75..4563e3721c0 100644
--- a/torch/_inductor/codegen/common.py
+++ b/torch/_inductor/codegen/common.py
@@ -2222,11 +2222,47 @@ def jinja2_env():
 
         return jinja2.Environment(
             undefined=jinja2.StrictUndefined,
+            bytecode_cache=jinja2.FileSystemBytecodeCache('/tmp/torch_jinja_cache', '%s.cache'),
+            #FileSystemBytecodeCache('/tmp/jinja_cache', '%s.cache')
         )
     except ImportError:
         return None
 
 
+from jinja2 import TemplateSyntaxError
+class DetailedTemplateSyntaxError(TemplateSyntaxError):
+    def __init__(self, original_error):
+        super().__init__(
+            original_error.message,
+            original_error.lineno,
+            original_error.name,
+            original_error.filename,
+        )
+        self.original_error = original_error
+
+    def __str__(self):
+        error_info = f"Error in template at line {self.lineno}\n"
+        error_info += f"Error message: {self.message}\n"
+        if hasattr(self.original_error, "source"):
+            lines = self.original_error.source.split("\n")
+            error_info += "Context:\n"
+            start = max(0, self.lineno - 2)
+            end = min(len(lines), self.lineno + 2)
+            for i in range(start, end):
+                if i == self.lineno - 1:
+                    error_info += f"{i + 1}: --> {lines[i]}\n"
+                    if hasattr(self.original_error, "column"):
+                        error_info += (
+                            "     "
+                            + " " * (self.original_error.column - 1)
+                            + "^\n"
+                        )
+                else:
+                    error_info += f"{i + 1}:     {lines[i]}\n"
+        return error_info
+
+import jinja2
+
 class KernelTemplate:
     """
     Base class for defining kernel templates.
@@ -2249,38 +2285,10 @@ class KernelTemplate:
         if env is None:
             return None
         env.filters["indent_except_first"] = KernelTemplate.indent_except_first
-        from jinja2 import TemplateSyntaxError
-
-        class DetailedTemplateSyntaxError(TemplateSyntaxError):
-            def __init__(self, original_error):
-                super().__init__(
-                    original_error.message,
-                    original_error.lineno,
-                    original_error.name,
-                    original_error.filename,
-                )
-                self.original_error = original_error
-
-            def __str__(self):
-                error_info = f"Error in template at line {self.lineno}\n"
-                error_info += f"Error message: {self.message}\n"
-                if hasattr(self.original_error, "source"):
-                    lines = self.original_error.source.split("\n")
-                    error_info += "Context:\n"
-                    start = max(0, self.lineno - 2)
-                    end = min(len(lines), self.lineno + 2)
-                    for i in range(start, end):
-                        if i == self.lineno - 1:
-                            error_info += f"{i + 1}: --> {lines[i]}\n"
-                            if hasattr(self.original_error, "column"):
-                                error_info += (
-                                    "     "
-                                    + " " * (self.original_error.column - 1)
-                                    + "^\n"
-                                )
-                        else:
-                            error_info += f"{i + 1}:     {lines[i]}\n"
-                return error_info
+
+        if isinstance(source, jinja2.Template):
+            source.environment = env
+            return source
 
         try:
             return env.from_string(source)
diff --git a/torch/_inductor/codegen/cpp_gemm_template.py b/torch/_inductor/codegen/cpp_gemm_template.py
index 69706565fc2..1ce3a3b0887 100644
--- a/torch/_inductor/codegen/cpp_gemm_template.py
+++ b/torch/_inductor/codegen/cpp_gemm_template.py
@@ -37,11 +37,12 @@ from .cpp_utils import (
     GemmBlocking,
     get_gemm_template_output_and_compute_dtype,
 )
+from jinja2 import Template
 
 
 log = logging.getLogger(__name__)
 
-GEMM_TEMPLATE_INIT_BLOCKING = r"""
+GEMM_TEMPLATE_INIT_BLOCKING = Template(r"""
     constexpr int64_t num_threads = {{num_threads}};
     constexpr int64_t N = {{N}};
     constexpr int64_t K = {{K}};
@@ -108,9 +109,9 @@ GEMM_TEMPLATE_INIT_BLOCKING = r"""
         Mt_blocks * Nt_blocks * Kt_blocks * {{num_threads}} >= Mr_blocks * Nr_blocks * Kr_blocks,
         "Not all partitions are assigned."
     );
-"""
+""")
 
-GEMM_TEMPLATE_MULTI_THREADS_PARAMS = r"""
+GEMM_TEMPLATE_MULTI_THREADS_PARAMS = Template(r"""
 const int tid = omp_get_thread_num();
 const int64_t k_group_id = tid / num_Kt_blocks;
 const int64_t k_slice_id = tid % num_Kt_blocks;
@@ -123,9 +124,9 @@ const int64_t n_block_end = std::min(n_block_start + Nt_blocks, Nr_blocks);
 const int64_t m_block_start = std::min(n_group_id * Mt_blocks, Mr_blocks);
 const int64_t m_block_end = std::min(m_block_start + Mt_blocks, Mr_blocks);
 const int64_t num_Mc_blocks_per_thread = (m_block_end - m_block_start + Mc_blocks - 1) / Mc_blocks;
-"""
+""")
 
-GEMM_TEMPLATE_SINGLE_THREAD_PARAMS = r"""
+GEMM_TEMPLATE_SINGLE_THREAD_PARAMS = Template(r"""
 constexpr int tid = 0;
 constexpr int64_t k_group_id = 0;
 constexpr int64_t k_slice_id = 0;
@@ -143,25 +144,25 @@ const int64_t m_block_end = Mr_blocks;
 constexpr int64_t num_Mc_blocks_per_thread = num_Mc_blocks;
 constexpr int64_t m_block_end = Mr_blocks;
 {%- endif %}
-"""
+""")
 
-GEMM_TEMPLATE_M_LOOP_PARAMS = r"""
+GEMM_TEMPLATE_M_LOOP_PARAMS = Template(r"""
 const int64_t my_mc_block_id = (mc_block_id + n_slice_id) % num_Mc_blocks_per_thread;
 const int64_t mc = m_block_start + my_mc_block_id * Mc_blocks;
 const int64_t m_start = mc * Mr;
 const int64_t m_end = std::min(std::min(mc + Mc_blocks, m_block_end) * Mr, M);
 const int64_t m_size = m_end - m_start;
-"""
+""")
 
-GEMM_TEMPLATE_N_LOOP_PARAMS = r"""
+GEMM_TEMPLATE_N_LOOP_PARAMS = Template(r"""
 const int64_t n_start = nc * Nr;
 const int64_t n_end = std::min(std::min(nc + Nc_blocks, n_block_end) * Nr, N);
 const int64_t n_size = n_end - n_start;
 // NB: assume we pad N, nc_block_end won't exceed padded N here.
 const int64_t nc_block_end = std::min(nc + Nc_blocks, n_block_end);
-"""
+""")
 
-GEMM_TEMPLATE = r"""
+GEMM_TEMPLATE = Template(r"""
 {{template.header().getvalue()}}
 
 {{micro_gemm.codegen_define(kernel)}}
@@ -282,7 +283,7 @@ extern "C" {{export_declaration}}
         {{ micro_gemm.codegen_finalize(kernel) }}
     }
 }
-"""
+""")
 
 
 def get_padded_n(n, block_n):
diff --git a/torch/_inductor/codegen/rocm/compile_command.py b/torch/_inductor/codegen/rocm/compile_command.py
index ee966f5fdd5..4561f7d926f 100644
--- a/torch/_inductor/codegen/rocm/compile_command.py
+++ b/torch/_inductor/codegen/rocm/compile_command.py
@@ -1,6 +1,7 @@
 # mypy: allow-untyped-defs
 import logging
 import os
+import functools
 from typing import List, Optional
 
 from torch._inductor import config
@@ -9,7 +10,7 @@ from torch._inductor.utils import is_linux
 
 log = logging.getLogger(__name__)
 
-
+@functools.cache
 def _rocm_include_paths(dst_file_ext: str) -> List[str]:
     from torch.utils import cpp_extension
 
@@ -44,6 +45,7 @@ def _rocm_include_paths(dst_file_ext: str) -> List[str]:
     return paths
 
 
+@functools.cache
 def _rocm_lib_options(dst_file_ext: str) -> List[str]:
     from torch.utils import cpp_extension
 
@@ -69,6 +71,7 @@ def _rocm_lib_options(dst_file_ext: str) -> List[str]:
     return opts
 
 
+@functools.cache
 def _rocm_compiler_options() -> List[str]:
     arch_list = config.rocm.arch or ["native"]
     gpu_arch_flags = [f"--offload-arch={arch}" for arch in arch_list]
@@ -100,6 +103,7 @@ def _rocm_compiler_options() -> List[str]:
     return opts
 
 
+@functools.cache
 def rocm_compiler() -> Optional[str]:
     if is_linux():
         if config.rocm.rocm_home:
